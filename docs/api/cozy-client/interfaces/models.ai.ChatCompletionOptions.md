[cozy-client](../README.md) / [models](../modules/models.md) / [ai](../modules/models.ai.md) / ChatCompletionOptions

# Interface: ChatCompletionOptions<>

[models](../modules/models.md).[ai](../modules/models.ai.md).ChatCompletionOptions

## Properties

### frequency_penalty

• **frequency_penalty**: `number`

Frequency penalty

*Defined in*

[packages/cozy-client/src/models/ai.js:61](https://github.com/cozy/cozy-client/blob/master/packages/cozy-client/src/models/ai.js#L61)

***

### max_tokens

• **max_tokens**: `number`

Maximum tokens to generate

*Defined in*

[packages/cozy-client/src/models/ai.js:59](https://github.com/cozy/cozy-client/blob/master/packages/cozy-client/src/models/ai.js#L59)

***

### model

• **model**: `string`

Optional model to use

*Defined in*

[packages/cozy-client/src/models/ai.js:56](https://github.com/cozy/cozy-client/blob/master/packages/cozy-client/src/models/ai.js#L56)

***

### presence_penalty

• **presence_penalty**: `number`

Presence penalty

*Defined in*

[packages/cozy-client/src/models/ai.js:60](https://github.com/cozy/cozy-client/blob/master/packages/cozy-client/src/models/ai.js#L60)

***

### stream

• **stream**: `boolean`

Whether to stream the response

*Defined in*

[packages/cozy-client/src/models/ai.js:55](https://github.com/cozy/cozy-client/blob/master/packages/cozy-client/src/models/ai.js#L55)

***

### temperature

• **temperature**: `number`

Temperature for randomness (0-2)

*Defined in*

[packages/cozy-client/src/models/ai.js:57](https://github.com/cozy/cozy-client/blob/master/packages/cozy-client/src/models/ai.js#L57)

***

### top_p

• **top_p**: `number`

Top-p sampling

*Defined in*

[packages/cozy-client/src/models/ai.js:58](https://github.com/cozy/cozy-client/blob/master/packages/cozy-client/src/models/ai.js#L58)
